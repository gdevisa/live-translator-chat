<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Live Translator Chat</title>
  <style>
    :root { --bg:#fff; --fg:#111; --muted:#777; --bubble:#f7f7f7; --bubbleMe:#eaf1ff; }
    body { margin:0; font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; background:var(--bg); color:var(--fg); }
    .wrap { max-width: 820px; margin: 0 auto; padding: 24px; }
    h2 { margin: 0 0 12px; }
    #chat { border:1px solid #ddd; border-radius:12px; padding:14px; min-height:360px; display:flex; flex-direction:column; gap:8px; overflow:auto; }
    .bubble { padding:10px 12px; border-radius:12px; max-width: 80%; white-space:pre-wrap; line-height:1.35; }
    .me { align-self:flex-end; background:var(--bubbleMe); }
    .ai { align-self:flex-start; background:var(--bubble); }
    .row { display:flex; gap:10px; margin-top:12px; }
    #text { flex:1; padding:10px; border:1px solid #ccc; border-radius:10px; font-size:15px; }
    button { padding:10px 14px; border:1px solid #ccc; border-radius:10px; background:#fff; cursor:pointer; font-size:15px; }
    button:active { transform: translateY(1px); }
    #status { margin-top:10px; color:var(--muted); font-size:13px; }
    .small { font-size:12px; color:var(--muted); }
    code { background:#f2f2f2; padding:2px 6px; border-radius:6px; }
  </style>
</head>
<body>
  <div class="wrap">
    <h2>ðŸŽ§ Live Translator Chat</h2>
    <div id="chat"></div>

    <div class="row">
      <input id="text" placeholder="Type context or chat (e.g., 'Iâ€™m learning Spanish for travel. Please translate to English.')" />
      <button id="send">Send</button>
      <button id="mic">Start Mic</button>
    </div>
    <div id="status">Ready.</div>
    <p class="small">
      While the mic is on: transcript + translation stream in this chat.<br/>
      When you stop: you'll receive <b>suggested replies</b> in target & source languages.
    </p>
  </div>

  <script>
    const MODEL = "gpt-4o-realtime-preview"; // adjust if needed

    const chat = document.getElementById("chat");
    const text = document.getElementById("text");
    const send = document.getElementById("send");
    const micBtn = document.getElementById("mic");
    const statusEl = document.getElementById("status");

    let pc, dc, micStream, started = false;

    function addBubble(role, msg, replaceLast=false) {
        if (replaceLast) {
          const last = chat.lastElementChild;
          if (last && last.dataset.role === role) {
            last.textContent = msg;
            chat.scrollTop = chat.scrollHeight;
            return last;
          }
        }
        const div = document.createElement("div");
        div.className = "bubble " + (role === "user" ? "me" : "ai");
        div.dataset.role = role;
        div.textContent = msg;
        chat.appendChild(div);
        chat.scrollTop = chat.scrollHeight;
        return div;
      }

      async function start() {
        micBtn.disabled = true;
        statusEl.textContent = "Startingâ€¦ (requesting mic & connecting)";
        try {
          console.log("[start] creating RTCPeerConnection");
          pc = new RTCPeerConnection();

          // Create data channel for events
          dc = pc.createDataChannel("oai-events");
          dc.onopen = () => {
            console.log("[dc] open");
            // Session instructions: live translate while mic is on
            dc.send(JSON.stringify({
              type: "session.update",
              session: {
                // text is default modality, but set explicitly to be safe
                modalities: ["text"],
                // server VAD helps model detect pauses/turns
                turn_detection: { type: "server_vad" },
                instructions:
                  "You are a live translating assistant. While the mic is on, transcribe the user's speech " +
                  "and translate it immediately using prior typed context. Be concise; while speaking, " +
                  "output only transcript + translation. When the mic stops, provide TWO short suggested " +
                  "replies in BOTH languages (target first, then source)."
              }
            }));

            // Replay visible chat as context
            const items = Array.from(chat.children).map(el => {
              const role = (el.dataset.role === "me") ? "user" : "assistant";
              const text = el.textContent || "";
              // Send existing messages as conversation items
              return { type:"message", role, content:[{ type:"output_text", text }] };
            });
            for (const it of items) {
              dc.send(JSON.stringify({ type: "conversation.item.create", item: it }));
            }

            // Kick off a streaming response (so we get ongoing transcript/translation)
            dc.send(JSON.stringify({
              type: "response.create",
              response: { modalities: ["text"] }
            }));

            console.log("[dc] session.update & response.create sent");
          };

          dc.onclose = () => console.log("[dc] close");
          dc.onerror  = (e) => console.error("[dc] error", e);

          // Log ALL events; also render text deltas from multiple possible event names
          dc.onmessage = (e) => {
            try {
              const ev = JSON.parse(e.data);
              // Always log the raw event type for debugging
              console.log("[dc] event:", ev.type, ev);

              // Unified handler for text streaming:
              // Some versions use response.output_text.delta; others: response.text.delta
              if (ev.type === "response.output_text.delta" || ev.type === "response.text.delta") {
                let last = chat.lastElementChild;
                if (!last || last.dataset.role !== "assistant") last = addBubble("assistant", "");
                last.textContent += ev.delta || "";
                chat.scrollTop = chat.scrollHeight;
                return;
              }

              // Optional: mark completion
              if (ev.type === "response.completed" || ev.type === "response.done") {
                console.log("[dc] response completed");
                return;
              }

              // (You can add special-cases here for VAD events, e.g. speech_started/speech_stopped)
            } catch (err) {
              console.error("[dc] parse error", err, e.data);
            }
          };

          pc.onconnectionstatechange = () => {
            console.log("[pc] state:", pc.connectionState);
            statusEl.textContent = "State: " + pc.connectionState;
          };

          // Mic
          micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          pc.addTrack(micStream.getTracks()[0], micStream);
          console.log("[start] mic track added");

          // Offer â†’ proxy â†’ answer
          const offer = await pc.createOffer();
          await pc.setLocalDescription(offer);
          console.log("[start] localDescription set; posting SDP to /api/realtime");
          const resp = await fetch(`/api/realtime?model=gpt-4o-realtime-preview`, {
            method: "POST",
            headers: { "Content-Type": "application/sdp" },
            body: offer.sdp
          });
          if (!resp.ok) {
            const txt = await resp.text();
            throw new Error(`Proxy /api/realtime failed: ${resp.status} ${txt}`);
          }
          const answer = { type: "answer", sdp: await resp.text() };
          await pc.setRemoteDescription(answer);
          console.log("[start] remoteDescription set; WebRTC established");

          addBubble("assistant", "â€¦listening & translatingâ€¦");
          statusEl.textContent = "ðŸŽ™ï¸ Listeningâ€¦ press Stop to end turn.";
          micBtn.textContent = "Stop Mic";
          started = true;
        } catch (err) {
          console.error(err);
          statusEl.textContent = "Error: " + String(err);
          try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}
          try { if (pc) pc.close(); } catch {}
          started = false;
          micBtn.textContent = "Start Mic";
        } finally {
          micBtn.disabled = false;
        }
      }

      async function stop() {
        try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}
        try { if (pc) pc.close(); } catch {}
        // Ask for suggestions only as a follow-up
        if (dc && dc.readyState === "open") {
          dc.send(JSON.stringify({
            type: "response.create",
            response: {
              modalities: ["text"],
              instructions:
                "Now that the speech turn is over, output ONLY the two short suggested replies in BOTH languages, " +
                "formatted exactly as:\n\n" +
                "Suggestions (<target_lang>):\nâ€¢ â€¦\nâ€¢ â€¦\n" +
                "Suggestions (<source_lang>):\nâ€¢ â€¦\nâ€¢ â€¦"
            }
          }));
        }
        statusEl.textContent = "Turn ended.";
        micBtn.textContent = "Start Mic";
        started = false;
      }

      micBtn.onclick = () => (started ? stop() : start());
    </script>
</body>
</html>
