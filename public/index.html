<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Live Translator Chat</title>
  <style>
    :root { --bg:#fff; --fg:#111; --muted:#777; --bubble:#f7f7f7; --bubbleMe:#eaf1ff; }
    body { margin:0; font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; background:var(--bg); color:var(--fg); }
    .wrap { max-width: 820px; margin: 0 auto; padding: 24px; }
    h2 { margin: 0 0 12px; }
    #chat { border:1px solid #ddd; border-radius:12px; padding:14px; min-height:360px; display:flex; flex-direction:column; gap:8px; overflow:auto; }
    .bubble { padding:10px 12px; border-radius:12px; max-width: 80%; white-space:pre-wrap; line-height:1.35; }
    .me { align-self:flex-end; background:var(--bubbleMe); }
    .ai { align-self:flex-start; background:var(--bubble); }
    .row { display:flex; gap:10px; margin-top:12px; }
    #text { flex:1; padding:10px; border:1px solid #ccc; border-radius:10px; font-size:15px; }
    button { padding:10px 14px; border:1px solid #ccc; border-radius:10px; background:#fff; cursor:pointer; font-size:15px; }
    button:active { transform: translateY(1px); }
    #status { margin-top:10px; color:var(--muted); font-size:13px; }
    .small { font-size:12px; color:var(--muted); }
    code { background:#f2f2f2; padding:2px 6px; border-radius:6px; }
  </style>
</head>
<body>
  <div class="wrap">
    <h2>ðŸŽ§ Live Translator Chat</h2>
    <div id="chat"></div>

    <div class="row">
      <input id="text" placeholder="Type context or chat (e.g., 'Iâ€™m learning Spanish for travel. Please translate to English.')" />
      <button id="send">Send</button>
      <button id="mic">Start Mic</button>
    </div>
    <div id="status">Ready.</div>
    <p class="small">
      While the mic is on: transcript + translation stream in this chat.<br/>
      When you stop: you'll receive <b>suggested replies</b> in target & source languages.
    </p>
  </div>

  <script>
    const MODEL = "gpt-4o-realtime-preview"; // adjust if needed

    const chat = document.getElementById("chat");
    const text = document.getElementById("text");
    const send = document.getElementById("send");
    const micBtn = document.getElementById("mic");
    const statusEl = document.getElementById("status");

    let pc, dc, micStream, started = false;

    function addBubble(role, msg, replaceLast=false) {
      if (replaceLast) {
        const last = chat.lastElementChild;
        if (last && last.dataset.role === role) {
          last.textContent = msg;
          return last;
        }
      }
      const div = document.createElement("div");
      div.className = "bubble " + (role === "user" ? "me" : "ai");
      div.dataset.role = role;
      div.textContent = msg;
      chat.appendChild(div);
      chat.scrollTop = chat.scrollHeight;
      return div;
    }

    // --- SEND a text message as context or normal chat turn
    send.onclick = async () => {
      const msg = text.value.trim();
      if (!msg) return;
      addBubble("user", msg);
      text.value = "";

      // If not connected to a realtime session yet, we can store this as "context" for later.
      // If already connected, push it into the conversation and request a reply.
      if (!dc || dc.readyState !== "open") {
        addBubble("assistant", "Context noted. (Start mic when ready.)");
        return;
      }

      // Tell the session about this user message
      dc.send(JSON.stringify({
        type: "conversation.item.create",
        item: {
          type: "message",
          role: "user",
          content: [{ type: "input_text", text: msg }]
        }
      }));

      // Ask model for a text reply (optional)
      dc.send(JSON.stringify({
        type: "response.create",
        response: { modalities: ["text"] }
      }));
    };

    // --- Start live: connect WebRTC, stream mic, and stream translation as text
    micBtn.onclick = async () => (started ? stop() : start());

    async function start() {
      // Peer connection + data channel
      pc = new RTCPeerConnection();
      dc = pc.createDataChannel("oai-events");

      // Stream incoming text deltas into the last assistant bubble
      dc.onmessage = (e) => {
        try {
          const m = JSON.parse(e.data);
          if (m.type === "response.output_text.delta") {
            const last = chat.lastElementChild;
            if (!last || last.dataset.role !== "assistant") addBubble("assistant", "");
            chat.lastElementChild.textContent += m.delta;
            chat.scrollTop = chat.scrollHeight;
          }
          if (m.type === "response.completed") {
            // Completed one response (we might trigger another after stop for suggestions)
          }
        } catch {}
      };

      pc.onconnectionstatechange = () => statusEl.textContent = "State: " + pc.connectionState;

      // Capture mic
      micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      pc.addTrack(micStream.getTracks()[0], micStream);

      // SDP offer â†’ your proxy â†’ SDP answer
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      const sdpResp = await fetch(`/api/realtime?model=${MODEL}`, {
        method: "POST",
        headers: { "Content-Type": "application/sdp" },
        body: offer.sdp
      });
      const answer = { type: "answer", sdp: await sdpResp.text() };
      await pc.setRemoteDescription(answer);

      dc.onopen = () => {
        // Always (re)state instructions for this session
        dc.send(JSON.stringify({
          type: "session.update",
          session: {
            instructions:
              "You are a live translating assistant. While the mic is on, transcribe the user's speech and " +
              "translate it immediately using the conversation context (the user's earlier typed messages). " +
              "Be concise; show only transcript + translation while speaking. " +
              "After the mic stops, provide TWO short suggested replies in BOTH languages (target first, then source)."
          }
        }));

        // If the user already typed context before starting, push it into the session as a user message
        // (replay the visible chat as user/assistant messages so the model has history).
        const items = Array.from(chat.children).map(el => {
          const role = (el.dataset.role === "me") ? "user" : "assistant";
          return { type:"message", role, content:[{ type:"output_text", text: el.textContent }] };
        });
        // Send prior visible conversation (optional but helpful)
        for (const it of items) {
          dc.send(JSON.stringify({ type: "conversation.item.create", item: it }));
        }

        // Kick off a streaming text response (transcript+translation) while mic is live
        dc.send(JSON.stringify({
          type: "response.create",
          response: { modalities: ["text"] }
        }));
      };

      addBubble("assistant", "â€¦listening & translatingâ€¦");
      statusEl.textContent = "ðŸŽ™ï¸ Listeningâ€¦ press Stop to end turn.";
      micBtn.textContent = "Stop Mic";
      started = true;
    }

    async function stop() {
      // End mic track and peer connection
      if (micStream) micStream.getTracks().forEach(t => t.stop());
      if (pc) pc.close();

      // After ending the turn, request suggestions as a FOLLOW-UP response
      if (dc && dc.readyState === "open") {
        dc.send(JSON.stringify({
          type: "response.create",
          response: {
            modalities: ["text"],
            instructions:
              "Now that the speech turn is over: output ONLY the two short suggested replies " +
              "in BOTH languages, formatted exactly as:\n\n" +
              "Suggestions (<target_lang>):\nâ€¢ â€¦\nâ€¢ â€¦\n" +
              "Suggestions (<source_lang>):\nâ€¢ â€¦\nâ€¢ â€¦"
          }
        }));
      } else {
        addBubble("assistant", "(ended)");
      }

      statusEl.textContent = "Turn ended.";
      micBtn.textContent = "Start Mic";
      started = false;
    }
  </script>
</body>
</html>
