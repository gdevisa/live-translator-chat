<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Live Translator (shared history + realtime finals)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#fff; --fg:#111; --muted:#777; --bubble:#f7f7f7; --bubbleMe:#eaf1ff; }
    body { margin:0; font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; background:var(--bg); color:var(--fg); }
    .wrap { max-width: 880px; margin: 0 auto; padding: 24px; }
    #chat { border:1px solid #ddd; border-radius:12px; padding:14px; min-height:380px; display:flex; flex-direction:column; gap:8px; overflow:auto; }
    .bubble { padding:10px 12px; border-radius:12px; max-width:80%; white-space:pre-wrap; line-height:1.35; }
    .me { align-self:flex-end; background:var(--bubbleMe); }
    .ai { align-self:flex-start; background:var(--bubble); }
    .row { display:flex; gap:10px; margin-top:12px; }
    #text { flex:1; padding:10px; border:1px solid #ccc; border-radius:10px; font-size:15px; }
    button { padding:10px 14px; border:1px solid #ccc; border-radius:10px; background:#fff; cursor:pointer; font-size:15px; }
    #status { margin-top:10px; color:var(--muted); font-size:13px; }
  </style>
  <link rel="icon" href="data:,">
</head>
<body>
  <div class="wrap">
    <h2>üéß Live Translator</h2>

    <div id="chat"></div>

    <div class="row">
      <input id="text" placeholder="Type chat/context (becomes conversation history)" />
      <button id="send">Send</button>
      <button id="mic">Start Mic</button>
    </div>

    <div id="status">Ready.</div>
  </div>

  <script>
    const MODEL = "gpt-4o-realtime-preview";

    const chatEl = document.getElementById("chat");
    const textEl = document.getElementById("text");
    const sendBtn = document.getElementById("send");
    const micBtn = document.getElementById("mic");
    const statusEl = document.getElementById("status");

    // ---- Single source of truth: chat memory ----
    const history = []; // [{role: "user"|"assistant", content: string}]

    // ---- WebRTC / Realtime state ----
    let pc, dc;
    let sessionOpen = false;
    let connecting = null;

    let micActive = false;
    let audioTransceiver = null;
    let silentTrack = null;
    let micStream = null;

    let currentResponseId = null;
    let awaitingAssistant = false;

    // Live mic buffers
    let liveTranscript = "";
    let micGuardArmed = false;
    let micBuffer = "";
    let ignoreDeltas = false;

    // Finals to commit on Stop
    let finalTranscriptText = "";
    let finalTranslationText = "";
    let currentSection = null; // "transcript" | "translation" | null

    // ---------- UI helpers ----------
    function addBubble(role, text, replaceLast=false) {
      if (replaceLast) {
        const last = chatEl.lastElementChild;
        if (last && last.dataset.role === (role === "user" ? "me" : "assistant")) {
          last.textContent = text;
          chatEl.scrollTop = chatEl.scrollHeight;
          return last;
        }
      }
      const div = document.createElement("div");
      div.className = "bubble " + (role === "user" ? "me" : "ai");
      div.dataset.role = role === "user" ? "me" : "assistant";
      div.textContent = text;
      chatEl.appendChild(div);
      chatEl.scrollTop = chatEl.scrollHeight;
      return div;
    }
    function latestAssistantBubble() {
      const els = Array.from(chatEl.children).filter(el => el.dataset.role === "assistant");
      return els.at(-1) || null;
    }

    // ---------- Audio helpers ----------
    function makeSilentTrack() {
      const AC = window.AudioContext || window.webkitAudioContext;
      const ac = new AC();
      const dst = ac.createMediaStreamDestination();
      const osc = ac.createOscillator(); osc.frequency.value = 0;
      const gain = ac.createGain(); gain.gain.value = 0.00001;
      osc.connect(gain).connect(dst); osc.start();
      return dst.stream.getAudioTracks()[0];
    }

    // ---------- DataChannel guards ----------
    function waitForDCOpen(channel, timeoutMs=10000) {
      return new Promise((resolve, reject) => {
        if (channel && channel.readyState === "open") return resolve();
        let done = false;
        const onOpen = () => { if (!done){ done=true; cleanup(); resolve(); } };
        const onClose= () => { if (!done){ done=true; cleanup(); reject(new Error("DC closed before open")); } };
        const onErr = e =>   { if (!done){ done=true; cleanup(); reject(e||new Error("DC error before open")); } };
        const timer = setTimeout(() => { if (!done){ done=true; cleanup(); reject(new Error("DC open timeout")); } }, timeoutMs);
        function cleanup(){ clearTimeout(timer); channel.removeEventListener("open",onOpen); channel.removeEventListener("close",onClose); channel.removeEventListener("error",onErr); }
        channel.addEventListener("open", onOpen, {once:true});
        channel.addEventListener("close", onClose, {once:true});
        channel.addEventListener("error", onErr, {once:true});
      });
    }

    // ---------- SDP exchange ----------
    async function negotiate() {
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      const resp = await fetch(`/api/realtime-sdp?model=${MODEL}`, {
        method: "POST",
        headers: { "Content-Type": "application/sdp" },
        body: offer.sdp
      });
      if (!resp.ok) throw new Error(await resp.text());
      const answer = { type: "answer", sdp: await resp.text() };
      await pc.setRemoteDescription(answer);
    }

    // ---------- Connect / setup Realtime ----------
    async function ensureConnection() {
      if (sessionOpen && dc && dc.readyState === "open") return;
      if (connecting) { await connecting; return; }

      connecting = (async () => {
        statusEl.textContent = "Connecting‚Ä¶";
        pc = new RTCPeerConnection({ iceServers: [{ urls: "stun:stun.l.google.com:19302" }] });

        dc = pc.createDataChannel("oai-events");
        dc.onmessage = (e) => {
          try {
            const ev = JSON.parse(e.data);

            if (ev.type === "response.created" || ev.type === "response.started") {
              if (ignoreDeltas) return;
              currentResponseId = ev.response?.id || ev.response_id || null;
              addBubble("assistant", "");
              awaitingAssistant = false;
              if (micActive) { micGuardArmed = true; micBuffer = ""; }
              return;
            }

            if (ev.type === "response.output_text.delta" || ev.type === "response.text.delta") {
              if (ignoreDeltas) return;
              let last = latestAssistantBubble();
              if (awaitingAssistant || !last) { last = addBubble("assistant", ""); awaitingAssistant = false; }
              const delta = ev.delta || "";
              last.textContent += delta; chatEl.scrollTop = chatEl.scrollHeight;

              if (micActive) {
                // Route streaming text into final buffers; do NOT update history here
                let chunk = delta;

                const hasTranscriptLabel = /Transcript:/i.test(chunk);
                const hasTranslationLabel = /Translation:/i.test(chunk);

                if (hasTranscriptLabel && hasTranslationLabel) {
                  const parts = chunk.split(/(Transcript:|Translation:)/i).filter(Boolean);
                  for (let i = 0; i < parts.length; i += 2) {
                    const label = (parts[i] || "").toLowerCase();
                    const text = parts[i + 1] || "";
                    if (label.includes("transcript")) {
                      currentSection = "transcript";
                      finalTranscriptText += text;
                    } else if (label.includes("translation")) {
                      currentSection = "translation";
                      finalTranslationText += text;
                    }
                  }
                } else if (hasTranscriptLabel) {
                  currentSection = "transcript";
                  chunk = chunk.replace(/^\s*Transcript:\s*/i, "");
                  finalTranscriptText += chunk;
                } else if (hasTranslationLabel) {
                  currentSection = "translation";
                  chunk = chunk.replace(/^\s*Translation:\s*/i, "");
                  finalTranslationText += chunk;
                } else {
                  if (currentSection === "transcript") {
                    finalTranscriptText += chunk;
                  } else if (currentSection === "translation") {
                    finalTranslationText += chunk;
                  } else {
                    currentSection = "transcript";
                    finalTranscriptText += chunk;
                  }
                }

                // Guard: first tokens must start with "Transcript:"
                if (micGuardArmed) {
                  micBuffer += delta;
                  const sample = micBuffer.trimStart();
                  if (sample.length >= 12) {
                    if (!/^Transcript:/i.test(sample)) {
                      cancelCurrentResponse();
                      last.textContent = ""; // clear wrong start bubble
                      awaitingAssistant = true;
                      micGuardArmed = true;
                      micBuffer = "";
                      reissueMicTurn(); // re-send strict instructions with history
                    } else {
                      micGuardArmed = false;
                    }
                  }
                }
              }
              return;
            }

            if (ev.type === "response.completed" || ev.type === "response.done") {
              currentResponseId = null;
              return;
            }
          } catch (err) {
            console.error("[dc] parse error", err, e.data);
          }
        };
        dc.onclose = () => { sessionOpen = false; };
        dc.onerror = (e) => console.error("[dc] error", e);

        // Add audio m-line first and bind a silent track (no renegotiation on mic start)
        audioTransceiver = pc.addTransceiver("audio", { direction: "sendrecv" });
        silentTrack = makeSilentTrack();
        await audioTransceiver.sender.replaceTrack(silentTrack);

        await negotiate();
        await waitForDCOpen(dc);
        sessionOpen = true;

        // Session defaults (no chat during mic turns unless asked)
        dc.send(JSON.stringify({
          type: "session.update",
          session: {
            modalities: ["text"],
            turn_detection: { type: "server_vad" },
            instructions:
              "You are a helpful assistant. Use the conversation memory in this session. " +
              "During mic turns, only produce Transcript:/Translation: as instructed."
          }
        }));

        // Push existing chat history into Realtime conversation memory
        await syncHistoryToRealtime();

        statusEl.textContent = "Connected.";
      })();

      await connecting;
      connecting = null;
    }

    function cancelCurrentResponse() {
      if (!dc || dc.readyState !== "open") return;
      if (!currentResponseId) return;
      try { dc.send(JSON.stringify({ type: "response.cancel", response: { id: currentResponseId } })); } catch {}
      currentResponseId = null;
    }

    // ---------- Realtime conversation sync (memory mirroring) ----------
    async function syncHistoryToRealtime() {
      if (!dc || dc.readyState !== "open") return;
      try {
        for (const m of history) {
          dc.send(JSON.stringify({
            type: "conversation.item.create",
            item: {
              type: "message",
              role: m.role,
              content: [{ type: "input_text", text: m.content }]
            }
          }));
        }
        dc.send(JSON.stringify({ type: "conversation.item.commit" }));
      } catch (e) { console.error("[syncHistoryToRealtime] error", e); }
    }
    function appendItemToRealtime(role, text) {
      if (!dc || dc.readyState !== "open") return;
      try {
        dc.send(JSON.stringify({
          type: "conversation.item.create",
          item: { type: "message", role, content: [{ type: "input_text", text }] }
        }));
        dc.send(JSON.stringify({ type: "conversation.item.commit" }));
      } catch (e) { console.error("[appendItemToRealtime] error", e); }
    }

    // ---------- TranscribeAgent instructions (runtime) ----------
    function buildTranscribeInstructions(h = []) {
      const TRANSCRIBE_INSTRUCTIONS = [
        "You are a TranscribeAgent responsible for real-time speech recognition and translation.",
        "Use the ongoing conversation memory already shared with you.",
        "During a mic turn:",
        "‚Ä¢ Output NOTHING until speech is detected.",
        "‚Ä¢ While speech is detected, stream ONLY:",
        "  Transcript: <partial transcript>",
        "  Translation: <partial translation>",
        "‚Ä¢ NEVER output general chat or commentary.",
        "‚Ä¢ Do NOT output suggestions. Those will be requested after the mic stops."
      ].join("\n");

      if (!Array.isArray(h) || !h.length) return TRANSCRIBE_INSTRUCTIONS;

      const lastFew = h.slice(-6);
      const formatted = lastFew.map(m => `${m.role.toUpperCase()}: ${m.content}`).join("\n");
      return `You have the following prior conversation context:\n${formatted}\n\n${TRANSCRIBE_INSTRUCTIONS}`;
    }

    async function reissueMicTurn() {
      const micInstructions = buildTranscribeInstructions(history);
      dc.send(JSON.stringify({
        type: "response.create",
        response: { modalities: ["text"], instructions: micInstructions }
      }));
    }

    // ---------- Typed chat flow ----------
    sendBtn.onclick = async () => {
      const msg = textEl.value.trim();
      if (!msg) return;
      if (micActive) await stopMic(true);

      addBubble("user", msg);
      history.push({ role: "user", content: msg });
      appendItemToRealtime("user", msg); // keep Realtime memory aligned

      textEl.value = ""; statusEl.textContent = "Thinking‚Ä¶";
      try {
        const r = await fetch("/api/chat", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ messages: history })
        });
        const data = await r.json();
        const reply = data.text || "(empty)";
        addBubble("assistant", reply);
        history.push({ role: "assistant", content: reply });
        appendItemToRealtime("assistant", reply); // align Realtime memory
        statusEl.textContent = "Ready.";
      } catch (e) {
        console.error(e); addBubble("assistant", "(chat error)"); statusEl.textContent = "Error.";
      }
    };

    // ---------- Mic (Realtime) ----------
    micBtn.onclick = () => micActive ? stopMic(false) : startMic();

    async function startMic() {
      micBtn.disabled = true; statusEl.textContent = "Starting mic‚Ä¶";
      try {
        await ensureConnection();
        ignoreDeltas = false;

        // Make sure Realtime has the latest chat memory
        await syncHistoryToRealtime();

        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const micTrack = micStream.getTracks()[0];
        await audioTransceiver.sender.replaceTrack(micTrack);

        micActive = true;
        liveTranscript = "";
        finalTranscriptText = "";
        finalTranslationText = "";
        currentSection = null;
        micGuardArmed = true;
        micBuffer = "";

        await waitForDCOpen(dc);

        awaitingAssistant = true;
        await reissueMicTurn(); // strict mic turn with shared history

        micBtn.textContent = "Stop Mic"; statusEl.textContent = "üéôÔ∏è Mic ON ‚Äî streaming‚Ä¶";
      } catch (e) {
        console.error(e); statusEl.textContent = "Mic error.";
        try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}
        micActive = false; micBtn.textContent = "Start Mic";
      } finally { micBtn.disabled = false; }
    }

    async function stopMic(quiet=false) {
      try { if (audioTransceiver && audioTransceiver.sender) await audioTransceiver.sender.replaceTrack(silentTrack || null); } catch {}
      try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}

      ignoreDeltas = true;
      cancelCurrentResponse();

      micActive = false; micGuardArmed = false; micBuffer = "";
      micBtn.textContent = "Start Mic";
      statusEl.textContent = quiet ? "Mic paused for chat." : "Mic OFF ‚Äî getting suggestions‚Ä¶";
      if (quiet) return;

      // === Commit finals to shared history before suggestions ===
      const committedTranscript = (finalTranscriptText || "").trim();
      const committedTranslation = (finalTranslationText || "").trim();

      if (committedTranscript) {
        const userMsg = `üéôÔ∏è Transcript: ${committedTranscript}`;
        history.push({ role: "user", content: userMsg });
        appendItemToRealtime("user", userMsg);
      }
      if (committedTranslation) {
        const assistantMsg = `üó£Ô∏è Translation: ${committedTranslation}`;
        history.push({ role: "assistant", content: assistantMsg });
        appendItemToRealtime("assistant", assistantMsg);
      }

      try {
        const r = await fetch("/api/mic-stop", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({
            transcript: committedTranscript,
            translation: committedTranslation, // optional but useful
            history
          })
        });
        const data = await r.json();
        const suggestions = data.suggestions || "(no suggestions)";

        addBubble("assistant", suggestions);
        history.push({ role: "assistant", content: suggestions });
        appendItemToRealtime("assistant", suggestions);

        statusEl.textContent = "Ready.";
      } catch (e) {
        console.error(e);
        addBubble("assistant", "(suggestions error)");
        statusEl.textContent = "Error.";
      }
    }
  </script>
</body>
</html>
