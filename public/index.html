<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Live Translator (Agents SDK handoff)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#fff; --fg:#111; --muted:#777; --bubble:#f7f7f7; --bubbleMe:#eaf1ff; }
    body { margin:0; font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; background:var(--bg); color:var(--fg); }
    .wrap { max-width: 880px; margin: 0 auto; padding: 24px; }
    #chat { border:1px solid #ddd; border-radius:12px; padding:14px; min-height:380px; display:flex; flex-direction:column; gap:8px; overflow:auto; }
    .bubble { padding:10px 12px; border-radius:12px; max-width:80%; white-space:pre-wrap; line-height:1.35; }
    .me { align-self:flex-end; background:var(--bubbleMe); }
    .ai { align-self:flex-start; background:var(--bubble); }
    .row { display:flex; gap:10px; margin-top:12px; }
    #text { flex:1; padding:10px; border:1px solid #ccc; border-radius:10px; font-size:15px; }
    button { padding:10px 14px; border:1px solid #ccc; border-radius:10px; background:#fff; cursor:pointer; font-size:15px; }
    #status { margin-top:10px; color:var(--muted); font-size:13px; }
  </style>
  <link rel="icon" href="data:,">
</head>
<body>
  <div class="wrap">
    <h2>ðŸŽ§ Live Translator (Agents SDK handoff)</h2>

    <div id="chat"></div>

    <div class="row">
      <input id="text" placeholder="Type context or chat (e.g., 'Iâ€™m practicing Spanish; translate my speech to English.')" />
      <button id="send">Send</button>
      <button id="mic">Start Mic</button>
    </div>

    <div id="status">Ready.</div>
  </div>

  <script>
    const MODEL = "gpt-4o-realtime-preview";

    const chatEl = document.getElementById("chat");
    const textEl = document.getElementById("text");
    const sendBtn = document.getElementById("send");
    const micBtn = document.getElementById("mic");
    const statusEl = document.getElementById("status");

    let pc, dc;
    let sessionOpen = false;
    let connecting = null;

    let micActive = false;
    let audioTransceiver = null;
    let silentTrack = null;
    let micStream = null;

    const history = [];
    let currentResponseId = null;
    let awaitingAssistant = false;

    let liveTranscript = "";
    let micGuardArmed = false;
    let micBuffer = "";

    function addBubble(role, text, replaceLast=false) {
      if (replaceLast) {
        const last = chatEl.lastElementChild;
        if (last && last.dataset.role === (role === "user" ? "me" : "assistant")) {
          last.textContent = text;
          chatEl.scrollTop = chatEl.scrollHeight;
          return last;
        }
      }
      const div = document.createElement("div");
      div.className = "bubble " + (role === "user" ? "me" : "ai");
      div.dataset.role = role === "user" ? "me" : "assistant";
      div.textContent = text;
      chatEl.appendChild(div);
      chatEl.scrollTop = chatEl.scrollHeight;
      return div;
    }
    function latestAssistantBubble() {
      const els = Array.from(chatEl.children).filter(el => el.dataset.role === "assistant");
      return els.at(-1) || null;
    }

    function makeSilentTrack() {
      const AC = window.AudioContext || window.webkitAudioContext;
      const ac = new AC();
      const dst = ac.createMediaStreamDestination();
      const osc = ac.createOscillator(); osc.frequency.value = 0;
      const gain = ac.createGain(); gain.gain.value = 0.00001;
      osc.connect(gain).connect(dst); osc.start();
      return dst.stream.getAudioTracks()[0];
    }

    function waitForDCOpen(channel, timeoutMs=10000) {
      return new Promise((resolve, reject) => {
        if (channel && channel.readyState === "open") return resolve();
        let done = false;
        const onOpen = () => { if (!done){ done=true; cleanup(); resolve(); } };
        const onClose= () => { if (!done){ done=true; cleanup(); reject(new Error("DC closed before open")); } };
        const onErr = e =>   { if (!done){ done=true; cleanup(); reject(e||new Error("DC error before open")); } };
        const timer = setTimeout(() => { if (!done){ done=true; cleanup(); reject(new Error("DC open timeout")); } }, timeoutMs);
        function cleanup(){ clearTimeout(timer); channel.removeEventListener("open",onOpen); channel.removeEventListener("close",onClose); channel.removeEventListener("error",onErr); }
        channel.addEventListener("open", onOpen, {once:true});
        channel.addEventListener("close", onClose, {once:true});
        channel.addEventListener("error", onErr, {once:true});
      });
    }

    async function negotiate() {
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      const resp = await fetch(`/api/realtime-sdp?model=${MODEL}`, {
        method: "POST",
        headers: { "Content-Type": "application/sdp" },
        body: offer.sdp
      });
      if (!resp.ok) throw new Error(await resp.text());
      const answer = { type: "answer", sdp: await resp.text() };
      await pc.setRemoteDescription(answer);
    }

    async function ensureConnection() {
      if (sessionOpen && dc && dc.readyState === "open") return;

      if (connecting) {
        await connecting; // wait ongoing connect
        return;
      }

      connecting = (async () => {
        statusEl.textContent = "Connectingâ€¦";
        pc = new RTCPeerConnection({ iceServers: [{ urls: "stun:stun.l.google.com:19302" }] });

        dc = pc.createDataChannel("oai-events");

        dc.onmessage = (e) => {
          try {
            const ev = JSON.parse(e.data);
            if (ev.type === "response.created" || ev.type === "response.started") {
              currentResponseId = ev.response?.id || ev.response_id || null;
              addBubble("assistant", "");
              awaitingAssistant = false;
              if (micActive) { micGuardArmed = true; micBuffer = ""; }
              return;
            }
            if (ev.type === "response.output_text.delta" || ev.type === "response.text.delta") {
              let last = latestAssistantBubble();
              if (awaitingAssistant || !last) { last = addBubble("assistant", ""); awaitingAssistant = false; }
              const delta = ev.delta || "";
              last.textContent += delta; chatEl.scrollTop = chatEl.scrollHeight;

              if (micActive) {
                if (/Transcript:/i.test(delta)) {
                  const cleaned = delta.replace(/^\s*Transcript:\s*/i, "");
                  liveTranscript += cleaned;
                } else {
                  liveTranscript += delta;
                }
                if (micGuardArmed) {
                  micBuffer += delta;
                  const sample = micBuffer.trimStart();
                  if (sample.length >= 12) {
                    if (!/^Transcript:/i.test(sample)) {
                      cancelCurrentResponse();
                      last.textContent = "";
                      awaitingAssistant = true;
                      micGuardArmed = true;
                      micBuffer = "";
                      dc.send(JSON.stringify({
                        type: "response.create",
                        response: {
                          modalities: ["text"],
                          instructions:
                            "MIC TURN STRICT.\n" +
                            "Output NOTHING until speech is detected.\n" +
                            "When speech is detected, stream ONLY:\n" +
                            "Transcript: <partial transcript>\n" +
                            "Translation: <partial translation>\n" +
                            "NEVER output generic chat during mic turn."
                        }
                      }));
                    } else {
                      micGuardArmed = false;
                    }
                  }
                }
              }
              return;
            }
            if (ev.type === "response.completed" || ev.type === "response.done") {
              currentResponseId = null;
              return;
            }
          } catch (err) { console.error("[dc] parse error", err, e.data); }
        };
        dc.onclose = () => { sessionOpen = false; };
        dc.onerror = (e) => console.error("[dc] error", e);

        // Add audio m-line BEFORE createOffer
        audioTransceiver = pc.addTransceiver("audio", { direction: "sendrecv" });
        silentTrack = makeSilentTrack();
        await audioTransceiver.sender.replaceTrack(silentTrack);

        await negotiate();

        // IMPORTANT: wait for DC open BEFORE any dc.send()
        await waitForDCOpen(dc);
        sessionOpen = true;

        // Now safe to send session.update
        dc.send(JSON.stringify({
          type: "session.update",
          session: {
            modalities: ["text"],
            turn_detection: { type: "server_vad" },
            instructions:
              "You are a helpful assistant. Only produce Transcript/Translation when explicitly instructed for a mic turn. " +
              "For normal text chat, provide a concise reply. Never include 'Transcript' or 'Translation' in normal chat."
          }
        }));
        statusEl.textContent = "Connected.";
      })();

      await connecting;
      connecting = null;
    }

    function cancelCurrentResponse() {
      if (!dc || dc.readyState !== "open") return;
      if (!currentResponseId) return;
      try { dc.send(JSON.stringify({ type: "response.cancel", response: { id: currentResponseId } })); } catch {}
      currentResponseId = null;
    }

    // ===== Typed chat via /api/chat =====
    sendBtn.onclick = async () => {
      const msg = textEl.value.trim();
      if (!msg) return;
      if (micActive) await stopMic(true);

      addBubble("user", msg);
      history.push({ role: "user", content: msg });
      textEl.value = ""; statusEl.textContent = "Thinkingâ€¦";
      try {
        const r = await fetch("/api/chat", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ messages: history })
        });
        const data = await r.json();
        const reply = data.text || "(empty)";
        addBubble("assistant", reply);
        history.push({ role: "assistant", content: reply });
        statusEl.textContent = "Ready.";
      } catch (e) {
        console.error(e); addBubble("assistant", "(chat error)"); statusEl.textContent = "Error.";
      }
    };

    // ===== Mic (Realtime) =====
    micBtn.onclick = () => micActive ? stopMic(false) : startMic();

    async function startMic() {
      micBtn.disabled = true; statusEl.textContent = "Starting micâ€¦";
      try {
        // Ensure PC/DC are connected AND DC is open before sending response.create
        await ensureConnection();

        // getUserMedia may prompt; only after connection is ready
        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const micTrack = micStream.getTracks()[0];

        // Swap silent -> mic (no renegotiation)
        await audioTransceiver.sender.replaceTrack(micTrack);
        micActive = true; liveTranscript = ""; micGuardArmed = true; micBuffer = "";

        // Double-check DC open (in case of slow networks)
        await waitForDCOpen(dc);

        // Now safe to send response.create
        awaitingAssistant = true;
        dc.send(JSON.stringify({
          type: "response.create",
          response: {
            modalities: ["text"],
            instructions:
              "MIC TURN.\n" +
              "Output NOTHING until speech is detected.\n" +
              "When speech is detected, stream ONLY:\n" +
              "Transcript: <partial transcript>\n" +
              "Translation: <partial translation>\n" +
              "NEVER output generic chat during mic turn."
          }
        }));

        micBtn.textContent = "Stop Mic"; statusEl.textContent = "ðŸŽ™ï¸ Mic ON â€” streamingâ€¦";
      } catch (e) {
        console.error(e); statusEl.textContent = "Mic error.";
        try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}
        micActive = false; micBtn.textContent = "Start Mic";
      } finally { micBtn.disabled = false; }
    }

    async function stopMic(quiet=false) {
      try { if (audioTransceiver && audioTransceiver.sender) await audioTransceiver.sender.replaceTrack(silentTrack || null); } catch {}
      try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}

      micActive = false; micGuardArmed = false; micBuffer = "";
      micBtn.textContent = "Start Mic";
      statusEl.textContent = quiet ? "Mic paused for chat." : "Mic OFF â€” getting suggestionsâ€¦";
      if (quiet) return;

      try {
        const r = await fetch("/api/mic-stop", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ transcript: liveTranscript.trim(), history })
        });
        const data = await r.json();
        addBubble("assistant", data.suggestions || "(no suggestions)");
        history.push({ role: "assistant", content: data.suggestions || "" });
        statusEl.textContent = "Ready.";
      } catch (e) {
        console.error(e); addBubble("assistant", "(suggestions error)"); statusEl.textContent = "Error.";
      }
    }
  </script>
</body>
</html>
