<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Live Translator Chat</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#fff; --fg:#111; --muted:#777; --bubble:#f7f7f7; --bubbleMe:#eaf1ff; }
    body { margin:0; font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; background:var(--bg); color:var(--fg); }
    .wrap { max-width: 880px; margin: 0 auto; padding: 24px; }
    h2 { margin: 0 0 12px; }
    #chat { border:1px solid #ddd; border-radius:12px; padding:14px; min-height:380px; display:flex; flex-direction:column; gap:8px; overflow:auto; }
    .bubble { padding:10px 12px; border-radius:12px; max-width: 80%; white-space:pre-wrap; line-height:1.35; }
    .me { align-self:flex-end; background:var(--bubbleMe); }
    .ai { align-self:flex-start; background:var(--bubble); }
    .row { display:flex; gap:10px; margin-top:12px; }
    #text { flex:1; padding:10px; border:1px solid #ccc; border-radius:10px; font-size:15px; }
    button { padding:10px 14px; border:1px solid #ccc; border-radius:10px; background:#fff; cursor:pointer; font-size:15px; }
    button:active { transform: translateY(1px); }
    #status { margin-top:10px; color:var(--muted); font-size:13px; }
    .small { font-size:12px; color:var(--muted); }
  </style>
</head>
<body>
  <div class="wrap">
    <h2>ðŸŽ§ Live Translator Chat</h2>

    <div id="chat"></div>

    <div class="row">
      <input id="text" placeholder="Type context or chat (e.g., 'Iâ€™m practicing Spanish; translate my speech to English.')" />
      <button id="send">Send</button>
      <button id="mic">Start Mic</button>
    </div>

    <div id="status">Ready.</div>
    <p class="small">
      Type a message first to set context (the model replies).<br/>
      Then press <b>Start Mic</b> â€” youâ€™ll see <em>Transcript</em> and <em>Translation</em> streaming.<br/>
      When you stop, youâ€™ll get <b>Suggestions</b> in target &amp; source languages.
    </p>
  </div>

  <script>
    // ===== Config =====
    const MODEL = "gpt-4o-realtime-preview";

    // ===== DOM =====
    const chat     = document.getElementById("chat");
    const text     = document.getElementById("text");
    const sendBtn  = document.getElementById("send");
    const micBtn   = document.getElementById("mic");
    const statusEl = document.getElementById("status");

    // ===== State =====
    let pc, dc, micStream;
    let sessionOpen = false;
    let micActive = false;
    let audioTransceiver = null; // single audio m-line we reuse

    const pendingContext = []; // messages typed before DC open

    // ===== UI helpers =====
    function addBubble(role, msg, replaceLast=false) {
      if (replaceLast) {
        const last = chat.lastElementChild;
        if (last && last.dataset.role === role) {
          last.textContent = msg;
          chat.scrollTop = chat.scrollHeight;
          return last;
        }
      }
      const div = document.createElement("div");
      div.className = "bubble " + (role === "user" ? "me" : "ai");
      div.dataset.role = role;
      div.textContent = msg;
      chat.appendChild(div);
      chat.scrollTop = chat.scrollHeight;
      return div;
    }

    function latestAssistantBubble() {
      const els = Array.from(chat.children).filter(el => el.dataset.role === "assistant");
      return els.at(-1) || null;
    }

    function waitForDataChannelOpen(channel, timeoutMs = 10000) {
      return new Promise((resolve, reject) => {
        if (channel && channel.readyState === "open") return resolve();
        let done = false;
        const onOpen = () => { if (!done) { done = true; cleanup(); resolve(); } };
        const onClose = () => { if (!done) { done = true; cleanup(); reject(new Error("DataChannel closed before open")); } };
        const onError = (e) => { if (!done) { done = true; cleanup(); reject(e || new Error("DataChannel error before open")); } };
        const timer = setTimeout(() => { if (!done) { done = true; cleanup(); reject(new Error("DataChannel open timeout")); } }, timeoutMs);
        function cleanup() {
          clearTimeout(timer);
          channel.removeEventListener("open", onOpen);
          channel.removeEventListener("close", onClose);
          channel.removeEventListener("error", onError);
        }
        channel.addEventListener("open", onOpen, { once: true });
        channel.addEventListener("close", onClose, { once: true });
        channel.addEventListener("error", onError, { once: true });
      });
    }

    async function negotiateWithServer() {
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      const resp = await fetch(`/api/realtime?model=${MODEL}`, {
        method: "POST",
        headers: { "Content-Type": "application/sdp" },
        body: offer.sdp
      });
      if (!resp.ok) {
        const txt = await resp.text();
        throw new Error(`Proxy /api/realtime failed: ${resp.status} ${txt}`);
      }
      const answer = { type: "answer", sdp: await resp.text() };
      await pc.setRemoteDescription(answer);
    }

    async function ensureConnection() {
      if (sessionOpen && dc && dc.readyState === "open") return;

      console.log("[rtc] creating RTCPeerConnection");
      pc = new RTCPeerConnection({
        iceServers: [{ urls: "stun:stun.l.google.com:19302" }] // basic STUN
      });

      // Prepare data channel
      dc = pc.createDataChannel("oai-events");

      // Text streaming handler
      dc.onmessage = (e) => {
        try {
          const ev = JSON.parse(e.data);
          if (ev.type === "response.output_text.delta" || ev.type === "response.text.delta") {
            let last = latestAssistantBubble();
            if (!last) last = addBubble("assistant", "");
            last.textContent += ev.delta || "";
            chat.scrollTop = chat.scrollHeight;
            return;
          }
        } catch (err) {
          console.error("[dc] parse error", err, e.data);
        }
      };

      dc.onclose = () => { console.log("[dc] close"); sessionOpen = false; };
      dc.onerror  = (e) => console.error("[dc] error", e);

      pc.onconnectionstatechange = () => {
        console.log("[pc] state:", pc.connectionState);
        statusEl.textContent = "State: " + pc.connectionState;
      };

      // Create ONE audio transceiver so initial SDP has an audio m-line
      audioTransceiver = pc.addTransceiver("audio", { direction: "sendrecv" });

      // First negotiation (no mic bound yet)
      await negotiateWithServer();

      // Wait for DC open before sending anything
      await waitForDataChannelOpen(dc);
      sessionOpen = true;
      console.log("[dc] open");

      // Session-wide instructions
      dc.send(JSON.stringify({
        type: "session.update",
        session: {
          modalities: ["text"],
          // server-side VAD helps detect speech turns
          turn_detection: { type: "server_vad" },
          instructions:
            "You are a live translating assistant. Use prior typed messages as context. " +
            "When the mic is ON, continuously stream ONLY the following two sections, updated as audio arrives:\n" +
            "Transcript: <partial transcript>\n" +
            "Translation: <partial translation>\n\n" +
            "When the mic is OFF and I ask, output ONLY the suggestions in BOTH languages (target first, then source), " +
            "formatted exactly as:\n\n" +
            "Suggestions (target):\nâ€¢ â€¦\nâ€¢ â€¦\n\n" +
            "Suggestions (source):\nâ€¢ â€¦\nâ€¢ â€¦"
        }
      }));

      // Replay visible chat as context for the session
      const items = Array.from(chat.children).map(el => {
        const role = (el.dataset.role === "me") ? "user" : "assistant";
        const t = el.textContent || "";
        return { type:"message", role, content:[{ type:"output_text", text: t }] };
      });
      for (const it of items) {
        dc.send(JSON.stringify({ type: "conversation.item.create", item: it }));
      }

      // Flush any queued pre-open messages
      for (const msg of pendingContext.splice(0)) {
        dc.send(JSON.stringify({
          type: "conversation.item.create",
          item: { type:"message", role:"user", content:[{ type:"input_text", text: msg }] }
        }));
      }

      statusEl.textContent = "Connected. You can chat or start mic.";
    }

    // ===== Text chat (context + normal Q/A) =====
    sendBtn.onclick = async () => {
      const msg = text.value.trim();
      if (!msg) return;

      addBubble("user", msg);
      text.value = "";

      if (!sessionOpen) {
        pendingContext.push(msg);
        try {
          statusEl.textContent = "Connectingâ€¦";
          await ensureConnection();
        } catch (err) {
          console.error(err);
          statusEl.textContent = "Error: " + String(err);
          return;
        }
      } else {
        dc.send(JSON.stringify({
          type: "conversation.item.create",
          item: { type:"message", role:"user", content:[{ type:"input_text", text: msg }] }
        }));
      }

      // Ask model to reply to this text turn
      dc.send(JSON.stringify({
        type: "response.create",
        response: { modalities: ["text"] }
      }));
    };

    // ===== Mic controls (streaming) =====
    micBtn.onclick = () => micActive ? stopMic() : startMic();

    async function startMic() {
      micBtn.disabled = true;
      statusEl.textContent = "Starting micâ€¦";
      try {
        await ensureConnection(); // DC must be open

        // Get mic and bind to existing transceiver (keeps single m-line)
        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const micTrack = micStream.getTracks()[0];
        await audioTransceiver.sender.replaceTrack(micTrack);

        // Renegotiate so server starts receiving our audio
        await negotiateWithServer();

        micActive = true;

        // Kick off streaming Transcript/Translation
        dc.send(JSON.stringify({
          type: "response.create",
          response: {
            modalities: ["text"],
            instructions:
              "While the mic is on, stream updates with exactly two sections, updated incrementally:\n" +
              "Transcript: <partial transcript>\n" +
              "Translation: <partial translation>\n" +
              "Do NOT include suggestions yet."
          }
        }));

        micBtn.textContent = "Stop Mic";
        statusEl.textContent = "ðŸŽ™ï¸ Mic ON â€” speaking will appear below.";
      } catch (err) {
        console.error(err);
        statusEl.textContent = "Mic error: " + String(err);
        try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}
        micActive = false;
        micBtn.textContent = "Start Mic";
      } finally {
        micBtn.disabled = false;
      }
    }

    async function stopMic() {
      // Unbind track to stop sending audio (keep session alive)
      try {
        if (audioTransceiver && audioTransceiver.sender) {
          await audioTransceiver.sender.replaceTrack(null);
        }
      } catch {}
      try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}

      micActive = false;
      micBtn.textContent = "Start Mic";
      statusEl.textContent = "Mic OFF â€” requesting suggestionsâ€¦";

      if (dc && dc.readyState === "open") {
        // Separate bubble for suggestions
        addBubble("assistant", "");
        dc.send(JSON.stringify({
          type: "response.create",
          response: {
            modalities: ["text"],
            instructions:
              "Now that the speech turn is over, output ONLY the suggestions in BOTH languages, formatted EXACTLY:\n\n" +
              "Suggestions (target):\nâ€¢ <short reply 1>\nâ€¢ <short reply 2>\n\n" +
              "Suggestions (source):\nâ€¢ <short reply 1>\nâ€¢ <short reply 2>"
          }
        }));
      }
    }
  </script>
</body>
</html>
