<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Live Translator Chat</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#fff; --fg:#111; --muted:#777; --bubble:#f7f7f7; --bubbleMe:#eaf1ff; }
    body { margin:0; font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; background:var(--bg); color:var(--fg); }
    .wrap { max-width: 880px; margin: 0 auto; padding: 24px; }
    h2 { margin: 0 0 12px; }
    #chat { border:1px solid #ddd; border-radius:12px; padding:14px; min-height:380px; display:flex; flex-direction:column; gap:8px; overflow:auto; }
    .bubble { padding:10px 12px; border-radius:12px; max-width: 80%; white-space:pre-wrap; line-height:1.35; }
    .me { align-self:flex-end; background:var(--bubbleMe); }
    .ai { align-self:flex-start; background:var(--bubble); }
    .row { display:flex; gap:10px; margin-top:12px; }
    #text { flex:1; padding:10px; border:1px solid #ccc; border-radius:10px; font-size:15px; }
    button { padding:10px 14px; border:1px solid #ccc; border-radius:10px; background:#fff; cursor:pointer; font-size:15px; }
    button:active { transform: translateY(1px); }
    #status { margin-top:10px; color:var(--muted); font-size:13px; }
    .small { font-size:12px; color:var(--muted); }
  </style>
</head>
<body>
  <div class="wrap">
    <h2>ðŸŽ§ Live Translator Chat</h2>

    <div id="chat"></div>

    <div class="row">
      <input id="text" placeholder="Type context or chat (e.g., 'Iâ€™m practicing Spanish; translate my speech to English.')" />
      <button id="send">Send</button>
      <button id="mic">Start Mic</button>
    </div>

    <div id="status">Ready.</div>
    <p class="small">
      Type a message first to set context (the model replies).<br/>
      Then press <b>Start Mic</b> â€” youâ€™ll see <em>Transcript</em> and <em>Translation</em> streaming.<br/>
      When you stop, youâ€™ll get <b>Suggestions</b> in target &amp; source languages.
    </p>
  </div>

  <script>
    // ===== Config =====
    const MODEL = "gpt-4o-realtime-preview";   // Realtime model

    // ===== DOM =====
    const chat     = document.getElementById("chat");
    const text     = document.getElementById("text");
    const sendBtn  = document.getElementById("send");
    const micBtn   = document.getElementById("mic");
    const statusEl = document.getElementById("status");

    // ===== RTC / Realtime session state =====
    let pc, dc, micStream;
    let sessionOpen = false;
    let micActive = false;

    // Queue of messages typed before the session exists
    const pendingContext = [];

    // ===== Helpers =====
    function addBubble(role, msg, replaceLast=false) {
      if (replaceLast) {
        const last = chat.lastElementChild;
        if (last && last.dataset.role === role) {
          last.textContent = msg;
          chat.scrollTop = chat.scrollHeight;
          return last;
        }
      }
      const div = document.createElement("div");
      div.className = "bubble " + (role === "user" ? "me" : "ai");
      div.dataset.role = role;
      div.textContent = msg;
      chat.appendChild(div);
      chat.scrollTop = chat.scrollHeight;
      return div;
    }

    function latestAssistantBubble() {
      const els = Array.from(chat.children).filter(el => el.dataset.role === "assistant");
      return els.at(-1) || null;
    }

    async function ensureConnection() {
      if (sessionOpen) return;

      console.log("[rtc] creating RTCPeerConnection");
      pc = new RTCPeerConnection();

      dc = pc.createDataChannel("oai-events");
      dc.onopen = () => {
        sessionOpen = true;
        console.log("[dc] open");

        // Session instructions: how to behave across the whole conversation
        dc.send(JSON.stringify({
          type: "session.update",
          session: {
            modalities: ["text"],
            // let the server detect pauses/turns while mic is on
            turn_detection: { type: "server_vad" },
            instructions:
              "You are a live translating assistant. Use prior typed messages as context. " +
              "When the mic is ON, continuously stream ONLY the following two sections, updating them as new audio arrives:\n" +
              "Transcript: <partial transcript>\n" +
              "Translation: <partial translation>\n\n" +
              "When the mic is OFF and I ask, output ONLY the suggestions in BOTH languages (target first, then source), " +
              "formatted as:\n\n" +
              "Suggestions (target):\nâ€¢ â€¦\nâ€¢ â€¦\n\n" +
              "Suggestions (source):\nâ€¢ â€¦\nâ€¢ â€¦"
          }
        }));

        // Replay visible chat so the model has full context
        const items = Array.from(chat.children).map(el => {
          const role = (el.dataset.role === "me") ? "user" : "assistant";
          const t = el.textContent || "";
          return { type:"message", role, content:[{ type:"output_text", text: t }] };
        });
        for (const it of items) {
          dc.send(JSON.stringify({ type: "conversation.item.create", item: it }));
        }

        // Flush queued context typed before connection
        for (const msg of pendingContext.splice(0)) {
          dc.send(JSON.stringify({
            type: "conversation.item.create",
            item: { type:"message", role:"user", content:[{ type:"input_text", text: msg }] }
          }));
        }
      };

      dc.onclose = () => { console.log("[dc] close"); sessionOpen = false; };
      dc.onerror  = (e) => console.error("[dc] error", e);

      // Handle all server events (including text deltas)
      dc.onmessage = (e) => {
        try {
          const ev = JSON.parse(e.data);
          // Streaming text deltas (names differ across versions)
          if (ev.type === "response.output_text.delta" || ev.type === "response.text.delta") {
            let last = latestAssistantBubble();
            if (!last) last = addBubble("assistant", "");
            last.textContent += ev.delta || "";
            chat.scrollTop = chat.scrollHeight;
            return;
          }
          if (ev.type === "response.completed" || ev.type === "response.done") {
            return;
          }
        } catch (err) {
          console.error("[dc] parse error", err, e.data);
        }
      };

      pc.onconnectionstatechange = () => {
        console.log("[pc] state:", pc.connectionState);
        statusEl.textContent = "State: " + pc.connectionState;
      };

      // IMPORTANT: Advertise an audio m-line BEFORE creating the offer,
      // so the very first SDP has an audio media section (fixes 400 invalid_offer).
      pc.addTransceiver("audio", { direction: "sendrecv" });
      // If your browser ever objects, you can use: { direction: "recvonly" }

      // Create SDP offer (now includes audio) and do the handshake
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      const resp = await fetch(`/api/realtime?model=${MODEL}`, {
        method: "POST",
        headers: { "Content-Type": "application/sdp" },
        body: offer.sdp
      });
      if (!resp.ok) {
        const txt = await resp.text();
        throw new Error(`Proxy /api/realtime failed: ${resp.status} ${txt}`);
      }
      const answer = { type: "answer", sdp: await resp.text() };
      await pc.setRemoteDescription(answer);

      console.log("[rtc] WebRTC data channel established");
      statusEl.textContent = "Connected. You can chat or start mic.";
    }

    // ===== Text chat (context + normal Q/A) =====
    sendBtn.onclick = async () => {
      const msg = text.value.trim();
      if (!msg) return;

      addBubble("user", msg);
      text.value = "";

      // If not connected, queue this as context and open the session
      if (!sessionOpen) {
        pendingContext.push(msg);
        try {
          statusEl.textContent = "Connectingâ€¦";
          await ensureConnection();
        } catch (err) {
          console.error(err);
          statusEl.textContent = "Error: " + String(err);
          return;
        }
      } else {
        // Already connected: push into conversation immediately
        dc.send(JSON.stringify({
          type: "conversation.item.create",
          item: { type:"message", role:"user", content:[{ type:"input_text", text: msg }] }
        }));
      }

      // Ask model to reply (useful to confirm context)
      dc.send(JSON.stringify({
        type: "response.create",
        response: { modalities: ["text"] }
      }));
    };

    // ===== Mic: start/stop live translation =====
    micBtn.onclick = () => micActive ? stopMic() : startMic();

    async function startMic() {
      micBtn.disabled = true;
      statusEl.textContent = "Starting micâ€¦";

      try {
        // Ensure we have an active session first
        if (!sessionOpen) {
          await ensureConnection();
        }

        // Add mic track
        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        pc.addTrack(micStream.getTracks()[0], micStream);
        micActive = true;

        // Start a streaming response for the live turn:
        // Stream only Transcript / Translation sections.
        dc.send(JSON.stringify({
          type: "response.create",
          response: {
            modalities: ["text"],
            instructions:
              "While the mic is on, stream updates with exactly two sections, updated incrementally:\n" +
              "Transcript: <partial transcript>\n" +
              "Translation: <partial translation>\n" +
              "Do NOT include suggestions yet."
          }
        }));

        micBtn.textContent = "Stop Mic";
        statusEl.textContent = "ðŸŽ™ï¸ Mic ON â€” speaking will appear below.";
      } catch (err) {
        console.error(err);
        statusEl.textContent = "Mic error: " + String(err);
        try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}
        micActive = false;
        micBtn.textContent = "Start Mic";
      } finally {
        micBtn.disabled = false;
      }
    }

    async function stopMic() {
      try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}
      micActive = false;
      micBtn.textContent = "Start Mic";
      statusEl.textContent = "Mic OFF â€” requesting suggestionsâ€¦";

      // Ask ONLY for suggestions (new assistant message)
      if (dc && dc.readyState === "open") {
        addBubble("assistant", ""); // separate bubble for suggestions
        dc.send(JSON.stringify({
          type: "response.create",
          response: {
            modalities: ["text"],
            instructions:
              "Now that the speech turn is over, output ONLY the suggestions in BOTH languages, formatted EXACTLY:\n\n" +
              "Suggestions (target):\nâ€¢ <short reply 1>\nâ€¢ <short reply 2>\n\n" +
              "Suggestions (source):\nâ€¢ <short reply 1>\nâ€¢ <short reply 2>"
          }
        }));
      }
    }
  </script>
</body>
</html>
