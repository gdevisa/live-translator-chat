<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Live Translator Chat</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#fff; --fg:#111; --muted:#777; --bubble:#f7f7f7; --bubbleMe:#eaf1ff; }
    body { margin:0; font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; background:var(--bg); color:var(--fg); }
    .wrap { max-width: 880px; margin: 0 auto; padding: 24px; }
    h2 { margin: 0 0 12px; }
    #chat { border:1px solid #ddd; border-radius:12px; padding:14px; min-height:380px; display:flex; flex-direction:column; gap:8px; overflow:auto; }
    .bubble { padding:10px 12px; border-radius:12px; max-width:80%; white-space:pre-wrap; line-height:1.35; }
    .me { align-self:flex-end; background:var(--bubbleMe); }
    .ai { align-self:flex-start; background:var(--bubble); }
    .row { display:flex; gap:10px; margin-top:12px; }
    #text { flex:1; padding:10px; border:1px solid #ccc; border-radius:10px; font-size:15px; }
    button { padding:10px 14px; border:1px solid #ccc; border-radius:10px; background:#fff; cursor:pointer; font-size:15px; }
    button:active { transform: translateY(1px); }
    #status { margin-top:10px; color:var(--muted); font-size:13px; }
    .small { font-size:12px; color:var(--muted); }
  </style>
  <!-- Inline favicon to avoid 404 -->
  <link rel="icon" href="data:,">
</head>
<body>
  <div class="wrap">
    <h2>🎧 Live Translator Chat</h2>

    <div id="chat"></div>

    <div class="row">
      <input id="text" placeholder="Type context or chat (e.g., 'I’m practicing Spanish; translate my speech to English.')" />
      <button id="send">Send</button>
      <button id="mic">Start Mic</button>
    </div>

    <div id="status">Ready.</div>
    <p class="small">
      Chat → normal replies. Start Mic → live <em>Transcript</em>/<em>Translation</em>. Stop Mic → <b>Suggestions</b>.
    </p>
  </div>

  <script>
    // ===== Config =====
    const MODEL   = "gpt-4o-realtime-preview";

    // ===== DOM =====
    const chat     = document.getElementById("chat");
    const text     = document.getElementById("text");
    const sendBtn  = document.getElementById("send");
    const micBtn   = document.getElementById("mic");
    const statusEl = document.getElementById("status");

    // ===== State =====
    let pc, dc;
    let sessionOpen = false;
    let micActive = false;

    let audioTransceiver = null; // single m=audio transceiver
    let silentTrack = null;      // always-bound silent track
    let micStream = null;        // user mic when active
    let connecting = null;       // prevent double connects

    const pendingContext = [];   // messages queued pre-DC-open
    let awaitingAssistant = false;     // start a new bubble on next response
    let currentResponseId = null;      // track active response
    let micGuardArmed = false;         // guard against generic replies during mic turn
    let micBuffer = "";                // collects first few deltas to validate format

    // ===== UI helpers =====
    function addBubble(role, msg, replaceLast=false) {
      if (replaceLast) {
        const last = chat.lastElementChild;
        if (last && last.dataset.role === (role === "user" ? "me" : "assistant")) {
          last.textContent = msg;
          chat.scrollTop = chat.scrollHeight;
          return last;
        }
      }
      const div = document.createElement("div");
      div.className = "bubble " + (role === "user" ? "me" : "ai");
      div.dataset.role = role === "user" ? "me" : "assistant";
      div.textContent = msg;
      chat.appendChild(div);
      chat.scrollTop = chat.scrollHeight;
      return div;
    }
    function latestAssistantBubble() {
      const els = Array.from(chat.children).filter(el => el.dataset.role === "assistant");
      return els.at(-1) || null;
    }
    function waitForDCOpen(channel, timeoutMs=10000) {
      return new Promise((resolve, reject) => {
        if (channel && channel.readyState === "open") return resolve();
        let done = false;
        const onOpen = () => { if (!done){ done=true; cleanup(); resolve(); } };
        const onClose= () => { if (!done){ done=true; cleanup(); reject(new Error("DC closed before open")); } };
        const onErr = e =>   { if (!done){ done=true; cleanup(); reject(e||new Error("DC error before open")); } };
        const timer = setTimeout(() => { if (!done){ done=true; cleanup(); reject(new Error("DC open timeout")); } }, timeoutMs);
        function cleanup(){ clearTimeout(timer); channel.removeEventListener("open",onOpen); channel.removeEventListener("close",onClose); channel.removeEventListener("error",onErr); }
        channel.addEventListener("open", onOpen, {once:true});
        channel.addEventListener("close", onClose, {once:true});
        channel.addEventListener("error", onErr, {once:true});
      });
    }

    async function negotiate() {
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      const resp = await fetch(`/api/realtime?model=${MODEL}`, {
        method: "POST",
        headers: { "Content-Type": "application/sdp" },
        body: offer.sdp
      });
      if (!resp.ok) {
        const txt = await resp.text();
        throw new Error(`Proxy /api/realtime failed: ${resp.status} ${txt}`);
      }
      const answer = { type: "answer", sdp: await resp.text() };
      await pc.setRemoteDescription(answer);
    }

    function makeSilentTrack() {
      const AC = window.AudioContext || window.webkitAudioContext;
      const ac = new AC();
      const dst = ac.createMediaStreamDestination();
      const osc = ac.createOscillator();
      osc.frequency.value = 0;
      const gain = ac.createGain();
      gain.gain.value = 0.00001;
      osc.connect(gain).connect(dst);
      osc.start();
      return dst.stream.getAudioTracks()[0];
    }

    async function ensureConnection() {
      if (sessionOpen && dc && dc.readyState === "open") return;
      if (connecting) return connecting;

      connecting = (async () => {
        console.log("[rtc] creating RTCPeerConnection");
        pc = new RTCPeerConnection({ iceServers: [{ urls: "stun:stun.l.google.com:19302" }] });

        // Data channel
        dc = pc.createDataChannel("oai-events");

        // Stream handler (supports both delta event names)
        dc.onmessage = (e) => {
          try {
            const ev = JSON.parse(e.data);
            if (ev.type === "response.created" || ev.type === "response.started") {
              currentResponseId = ev.response?.id || ev.response_id || null;
              // new response → new assistant bubble
              addBubble("assistant", "");
              awaitingAssistant = false;
              // reset mic buffer whenever a response starts
              if (micActive) {
                micGuardArmed = true;
                micBuffer = "";
              }
              return;
            }

            if (ev.type === "response.output_text.delta" || ev.type === "response.text.delta") {
              let last = latestAssistantBubble();
              if (awaitingAssistant || !last) {
                last = addBubble("assistant", "");
                awaitingAssistant = false;
              }

              const delta = ev.delta || "";
              last.textContent += delta;
              chat.scrollTop = chat.scrollHeight;

              // ===== Mic-turn guard: ensure the FIRST content starts with "Transcript:" =====
              if (micActive && micGuardArmed) {
                micBuffer += delta;
                const sample = micBuffer.trimStart();
                // If we've got enough characters to decide and it's NOT starting with "Transcript:",
                // cancel and re-issue with stricter instruction.
                if (sample.length >= 12) {
                  if (!/^Transcript:/i.test(sample)) {
                    console.warn("[guard] Mic turn started producing generic text. Cancelling & re-issuing mic instructions.");
                    cancelCurrentResponse();
                    // Replace current bubble text with a clean header to avoid leftover generic text.
                    last.textContent = "";
                    // Re-issue strict mic response
                    awaitingAssistant = true;
                    micGuardArmed = true;
                    micBuffer = "";
                    dc.send(JSON.stringify({
                      type: "response.create",
                      response: {
                        modalities: ["text"],
                        instructions:
                          "MIC TURN STRICT MODE.\n" +
                          "Rules:\n" +
                          "1) Do NOT output anything until speech is detected from the WebRTC audio.\n" +
                          "2) When speech is detected, output ONLY two sections and nothing else:\n" +
                          "Transcript: <partial transcript>\n" +
                          "Translation: <partial translation>\n" +
                          "3) NEVER produce general chat text or greetings during a mic turn.\n" +
                          "4) Do not repeat suggestions; suggestions come only after mic stops."
                      }
                    }));
                  } else {
                    // We got Transcript: — disarm the guard
                    micGuardArmed = false;
                  }
                }
              }
              return;
            }

            if (ev.type === "response.completed" || ev.type === "response.done") {
              currentResponseId = null;
              return;
            }
          } catch (err) {
            console.error("[dc] parse error", err, e.data);
          }
        };

        dc.onclose = () => { console.log("[dc] close"); sessionOpen = false; };
        dc.onerror = (e) => console.error("[dc] error", e);

        pc.onconnectionstatechange = () => {
          console.log("[pc] state:", pc.connectionState);
          statusEl.textContent = "State: " + pc.connectionState;
        };

        // Single audio transceiver + bind SILENT track from the start (no renegotiation later)
        audioTransceiver = pc.addTransceiver("audio", { direction: "sendrecv" });
        silentTrack = makeSilentTrack();
        await audioTransceiver.sender.replaceTrack(silentTrack);

        // One-time negotiation
        await negotiate();

        // Wait for DC open
        await waitForDCOpen(dc);
        sessionOpen = true;
        console.log("[dc] open");

        // Session-wide behavior (default: act like normal assistant; mic turns are opt-in)
        dc.send(JSON.stringify({
          type: "session.update",
          session: {
            modalities: ["text"],
            turn_detection: { type: "server_vad" },
            instructions:
              "You are a helpful assistant for chat. " +
              "Only produce Transcript/Translation when explicitly instructed for a mic turn. " +
              "For normal text chat, provide a concise helpful reply, and NEVER include 'Transcript' or 'Translation'."
          }
        }));

        // Replay visible chat as context
        const items = Array.from(chat.children).map(el => {
          const role = (el.dataset.role === "me") ? "user" : "assistant";
          const t = el.textContent || "";
          return { type:"message", role, content:[{ type:"output_text", text: t }] };
        });
        for (const it of items) {
          dc.send(JSON.stringify({ type: "conversation.item.create", item: it }));
        }

        // Flush queued context
        for (const msg of pendingContext.splice(0)) {
          dc.send(JSON.stringify({
            type: "conversation.item.create",
            item: { type:"message", role:"user", content:[{ type:"input_text", text: msg }] }
          }));
        }

        statusEl.textContent = "Connected. You can chat or start mic.";
      })().finally(() => { connecting = null; });

      return connecting;
    }

    function cancelCurrentResponse() {
      if (!dc || dc.readyState !== "open") return;
      if (!currentResponseId) return;
      try {
        dc.send(JSON.stringify({
          type: "response.cancel",
          response: { id: currentResponseId }
        }));
      } catch {}
      currentResponseId = null;
    }

    // ===== Text chat (normal Q/A) =====
    sendBtn.onclick = async () => {
      const msg = text.value.trim();
      if (!msg) return;

      // If mic is on, quietly stop it first (no suggestions) to avoid mixed streams
      if (micActive) {
        await stopMic(true);
      }

      addBubble("user", msg);
      text.value = "";

      if (!sessionOpen) {
        pendingContext.push(msg);
        try {
          statusEl.textContent = "Connecting…";
          await ensureConnection();
        } catch (err) {
          console.error(err);
          statusEl.textContent = "Error: " + String(err);
          return;
        }
      } else {
        dc.send(JSON.stringify({
          type: "conversation.item.create",
          item: { type:"message", role:"user", content:[{ type:"input_text", text: msg }] }
        }));
      }

      // Cancel any live response left over, then request a normal chat reply
      cancelCurrentResponse();
      awaitingAssistant = true;
      dc.send(JSON.stringify({
        type: "response.create",
        response: {
          modalities: ["text"],
          // Force normal chat behavior for this turn
          instructions:
            "Provide a helpful, concise assistant reply to the last user message. " +
            "Do NOT include 'Transcript' or 'Translation' in this text turn."
        }
      }));
    };

    // ===== Mic controls (no renegotiation; swap tracks) =====
    micBtn.onclick = () => micActive ? stopMic(false) : startMic();

    async function startMic() {
      micBtn.disabled = true;
      statusEl.textContent = "Starting mic…";
      try {
        await ensureConnection(); // DC must be open

        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const micTrack = micStream.getTracks()[0];

        if (!audioTransceiver || !audioTransceiver.sender) {
          throw new Error("Audio transceiver not ready.");
        }

        // Cancel any existing response to avoid interleaving
        cancelCurrentResponse();

        // SILENT -> MIC (no renegotiation)
        await audioTransceiver.sender.replaceTrack(micTrack);
        micActive = true;
        micGuardArmed = true;
        micBuffer = "";

        // Start a live streaming response (single growing bubble)
        awaitingAssistant = true;
        dc.send(JSON.stringify({
          type: "response.create",
          response: {
            modalities: ["text"],
            instructions:
              "MIC TURN.\n" +
              "Output NOTHING until speech is detected from WebRTC audio.\n" +
              "When speech is detected, output ONLY these two sections, streaming as they update:\n" +
              "Transcript: <partial transcript>\n" +
              "Translation: <partial translation>\n" +
              "NEVER produce general chat or any other text during a mic turn."
          }
        }));

        micBtn.textContent = "Stop Mic";
        statusEl.textContent = "🎙️ Mic ON — speaking will appear below.";
      } catch (err) {
        console.error(err);
        statusEl.textContent = "Mic error: " + String(err);
        try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}
        micActive = false;
        micBtn.textContent = "Start Mic";
      } finally {
        micBtn.disabled = false;
      }
    }

    async function stopMic(quiet=false) {
      // MIC -> SILENT (no renegotiation)
      try {
        if (audioTransceiver && audioTransceiver.sender) {
          await audioTransceiver.sender.replaceTrack(silentTrack || null);
        }
      } catch {}
      try { if (micStream) micStream.getTracks().forEach(t => t.stop()); } catch {}

      micActive = false;
      micGuardArmed = false;
      micBuffer = "";

      micBtn.textContent = "Start Mic";
      statusEl.textContent = quiet ? "Mic paused for chat." : "Mic OFF — requesting suggestions…";

      if (quiet) return; // no suggestions when stopping due to chat send

      if (dc && dc.readyState === "open") {
        // Cancel any leftover streaming response before suggestions
        cancelCurrentResponse();

        addBubble("assistant", ""); // separate bubble for suggestions
        awaitingAssistant = false;
        dc.send(JSON.stringify({
          type: "response.create",
          response: {
            modalities: ["text"],
            instructions:
              "Mic turn ended. Output ONLY the suggestions in BOTH languages, formatted EXACTLY:\n\n" +
              "Suggestions (target):\n• <short reply 1>\n• <short reply 2>\n\n" +
              "Suggestions (source):\n• <short reply 1>\n• <short reply 2>"
          }
        }));
      }
    }
  </script>
</body>
</html>
